{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "833af78a-ec8e-478f-8421-4df816f7db0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTh1_96_96', model='LLMMixer', data='ETTh1', root_path='/home/kowsher/tabllm', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=0, pred_len=96, seasonal_patterns='Monthly', inverse=False, llm_path='FacebookAI/roberta-base', tokenizer_path='FacebookAI/roberta-base', top_k=5, num_kernels=6, enc_in=7, dec_in=7, c_out=7, d_model=16, n_heads=4, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, channel_independence=1, decomp_method='moving_avg', use_norm=1, down_sampling_layers=3, down_sampling_window=2, down_sampling_method='avg', use_future_temporal_feature=0, mask_rate=0.25, anomaly_ratio=0.25, num_workers=10, itr=1, train_epochs=10, batch_size=16, patience=10, learning_rate=0.01, des='Exp', loss='MSE', lradj='TST', pct_start=0.2, use_amp=False, comment='none', use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1', p_hidden_dims=[128, 128], p_hidden_layers=2)\n",
      "Use GPU: cuda:0\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      ">>>>>>>start training : long_term_forecast_ETTh1_96_96_none_LLMMixer_ETTh1_sl96_pl96_dm16_nh4_el2_dl1_df32_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 8449\n",
      "val 2785\n",
      "test 2785\n",
      "\titers: 100, epoch: 1 | loss: 0.4917792\n",
      "\tspeed: 0.0983s/iter; left time: 509.3072s\n",
      "\titers: 200, epoch: 1 | loss: 0.3945865\n",
      "\tspeed: 0.0875s/iter; left time: 444.7195s\n",
      "\titers: 300, epoch: 1 | loss: 0.3944985\n",
      "\tspeed: 0.0849s/iter; left time: 422.8530s\n",
      "\titers: 400, epoch: 1 | loss: 0.3957745\n",
      "\tspeed: 0.0849s/iter; left time: 414.5804s\n",
      "\titers: 500, epoch: 1 | loss: 0.2882191\n",
      "\tspeed: 0.0843s/iter; left time: 403.1216s\n",
      "Epoch: 1 cost time: 46.369423627853394\n",
      "Epoch: 1, Steps: 528 | Train Loss: 0.4025009 Vali Loss: 0.7341680 Test Loss: 0.3990791\n",
      "Validation loss decreased (inf --> 0.734168).  Saving model ...\n",
      "Updating learning rate to 0.00520714674841976\n",
      "\titers: 100, epoch: 2 | loss: 0.4066612\n",
      "\tspeed: 0.2874s/iter; left time: 1337.1804s\n",
      "\titers: 200, epoch: 2 | loss: 0.3185660\n",
      "\tspeed: 0.0867s/iter; left time: 394.8404s\n",
      "\titers: 300, epoch: 2 | loss: 0.3843989\n",
      "\tspeed: 0.0872s/iter; left time: 388.3990s\n",
      "\titers: 400, epoch: 2 | loss: 0.3160379\n",
      "\tspeed: 0.0892s/iter; left time: 388.0923s\n",
      "\titers: 500, epoch: 2 | loss: 0.4589095\n",
      "\tspeed: 0.0902s/iter; left time: 383.5923s\n",
      "Epoch: 2 cost time: 46.640549421310425\n",
      "Epoch: 2, Steps: 528 | Train Loss: 0.3707505 Vali Loss: 0.7322576 Test Loss: 0.4113711\n",
      "Validation loss decreased (0.734168 --> 0.732258).  Saving model ...\n",
      "Updating learning rate to 0.009999998617101907\n",
      "\titers: 100, epoch: 3 | loss: 0.3023243\n",
      "\tspeed: 0.2974s/iter; left time: 1226.9221s\n",
      "\titers: 200, epoch: 3 | loss: 0.2711970\n",
      "\tspeed: 0.0898s/iter; left time: 361.3365s\n",
      "\titers: 300, epoch: 3 | loss: 0.4236498\n",
      "\tspeed: 0.0877s/iter; left time: 344.3622s\n",
      "\titers: 400, epoch: 3 | loss: 0.3887454\n",
      "\tspeed: 0.0896s/iter; left time: 342.7456s\n",
      "\titers: 500, epoch: 3 | loss: 0.3420490\n",
      "\tspeed: 0.0891s/iter; left time: 331.7509s\n",
      "Epoch: 3 cost time: 47.15513205528259\n",
      "Epoch: 3, Steps: 528 | Train Loss: 0.3649180 Vali Loss: 0.7138233 Test Loss: 0.3965764\n",
      "Validation loss decreased (0.732258 --> 0.713823).  Saving model ...\n",
      "Updating learning rate to 0.009617974812471862\n",
      "\titers: 100, epoch: 4 | loss: 0.3144492\n",
      "\tspeed: 0.3070s/iter; left time: 1104.4533s\n",
      "\titers: 200, epoch: 4 | loss: 0.3050549\n",
      "\tspeed: 0.0938s/iter; left time: 328.0874s\n",
      "\titers: 300, epoch: 4 | loss: 0.3498230\n",
      "\tspeed: 0.0930s/iter; left time: 315.9690s\n",
      "\titers: 400, epoch: 4 | loss: 0.3028170\n",
      "\tspeed: 0.0917s/iter; left time: 302.1818s\n",
      "\titers: 500, epoch: 4 | loss: 0.2849405\n",
      "\tspeed: 0.0900s/iter; left time: 287.8513s\n",
      "Epoch: 4 cost time: 48.79523754119873\n",
      "Epoch: 4, Steps: 528 | Train Loss: 0.3606967 Vali Loss: 0.7029669 Test Loss: 0.3923919\n",
      "Validation loss decreased (0.713823 --> 0.702967).  Saving model ...\n",
      "Updating learning rate to 0.008532909249507597\n",
      "\titers: 100, epoch: 5 | loss: 0.3979703\n",
      "\tspeed: 0.3016s/iter; left time: 925.5395s\n",
      "\titers: 200, epoch: 5 | loss: 0.2954434\n",
      "\tspeed: 0.0912s/iter; left time: 270.7868s\n",
      "\titers: 300, epoch: 5 | loss: 0.2911673\n",
      "\tspeed: 0.0908s/iter; left time: 260.6083s\n",
      "\titers: 400, epoch: 5 | loss: 0.2942167\n",
      "\tspeed: 0.0866s/iter; left time: 239.8366s\n",
      "\titers: 500, epoch: 5 | loss: 0.3289764\n",
      "\tspeed: 0.0910s/iter; left time: 242.8656s\n",
      "Epoch: 5 cost time: 47.59952759742737\n",
      "Epoch: 5, Steps: 528 | Train Loss: 0.3539243 Vali Loss: 0.7175945 Test Loss: 0.3852153\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.006909993324026596\n",
      "\titers: 100, epoch: 6 | loss: 0.3543544\n",
      "\tspeed: 0.2855s/iter; left time: 725.4872s\n",
      "\titers: 200, epoch: 6 | loss: 0.2702494\n",
      "\tspeed: 0.0896s/iter; left time: 218.7535s\n",
      "\titers: 300, epoch: 6 | loss: 0.2880955\n",
      "\tspeed: 0.0911s/iter; left time: 213.2047s\n",
      "\titers: 400, epoch: 6 | loss: 0.3242707\n",
      "\tspeed: 0.0920s/iter; left time: 206.1433s\n",
      "\titers: 500, epoch: 6 | loss: 0.3976877\n",
      "\tspeed: 0.0913s/iter; left time: 195.4148s\n",
      "Epoch: 6 cost time: 47.71373987197876\n",
      "Epoch: 6, Steps: 528 | Train Loss: 0.3440448 Vali Loss: 0.7052235 Test Loss: 0.3777577\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.004996301273913843\n",
      "\titers: 100, epoch: 7 | loss: 0.3202177\n",
      "\tspeed: 0.3020s/iter; left time: 607.9887s\n",
      "\titers: 200, epoch: 7 | loss: 0.3028615\n",
      "\tspeed: 0.0935s/iter; left time: 178.8744s\n",
      "\titers: 300, epoch: 7 | loss: 0.3420285\n",
      "\tspeed: 0.0883s/iter; left time: 160.0485s\n",
      "\titers: 400, epoch: 7 | loss: 0.2982474\n",
      "\tspeed: 0.0856s/iter; left time: 146.6538s\n",
      "\titers: 500, epoch: 7 | loss: 0.2703100\n",
      "\tspeed: 0.0866s/iter; left time: 139.7574s\n",
      "Epoch: 7 cost time: 47.22191619873047\n",
      "Epoch: 7, Steps: 528 | Train Loss: 0.3341917 Vali Loss: 0.6982239 Test Loss: 0.3730866\n",
      "Validation loss decreased (0.702967 --> 0.698224).  Saving model ...\n",
      "Updating learning rate to 0.0030831753661373714\n",
      "\titers: 100, epoch: 8 | loss: 0.2825237\n",
      "\tspeed: 0.2935s/iter; left time: 435.8053s\n",
      "\titers: 200, epoch: 8 | loss: 0.2363315\n",
      "\tspeed: 0.0878s/iter; left time: 121.5899s\n",
      "\titers: 300, epoch: 8 | loss: 0.2933947\n",
      "\tspeed: 0.0895s/iter; left time: 115.0582s\n",
      "\titers: 400, epoch: 8 | loss: 0.2520040\n",
      "\tspeed: 0.0906s/iter; left time: 107.3485s\n",
      "\titers: 500, epoch: 8 | loss: 0.3208239\n",
      "\tspeed: 0.0912s/iter; left time: 98.9861s\n",
      "Epoch: 8 cost time: 47.518367290496826\n",
      "Epoch: 8, Steps: 528 | Train Loss: 0.3246259 Vali Loss: 0.6898661 Test Loss: 0.3784926\n",
      "Validation loss decreased (0.698224 --> 0.689866).  Saving model ...\n",
      "Updating learning rate to 0.001461871677626608\n",
      "\titers: 100, epoch: 9 | loss: 0.2886084\n",
      "\tspeed: 0.3032s/iter; left time: 290.2048s\n",
      "\titers: 200, epoch: 9 | loss: 0.3141965\n",
      "\tspeed: 0.0882s/iter; left time: 75.6143s\n",
      "\titers: 300, epoch: 9 | loss: 0.3236958\n",
      "\tspeed: 0.0925s/iter; left time: 70.0475s\n",
      "\titers: 400, epoch: 9 | loss: 0.3067602\n",
      "\tspeed: 0.0904s/iter; left time: 59.3831s\n",
      "\titers: 500, epoch: 9 | loss: 0.3002172\n",
      "\tspeed: 0.0923s/iter; left time: 51.3932s\n",
      "Epoch: 9 cost time: 48.14889860153198\n",
      "Epoch: 9, Steps: 528 | Train Loss: 0.3157215 Vali Loss: 0.6974072 Test Loss: 0.3768456\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0003792189978027836\n",
      "\titers: 100, epoch: 10 | loss: 0.3423507\n",
      "\tspeed: 0.3013s/iter; left time: 129.2381s\n",
      "\titers: 200, epoch: 10 | loss: 0.2728983\n",
      "\tspeed: 0.0919s/iter; left time: 30.2443s\n",
      "\titers: 300, epoch: 10 | loss: 0.2649189\n",
      "\tspeed: 0.0917s/iter; left time: 20.9985s\n",
      "\titers: 400, epoch: 10 | loss: 0.2794529\n",
      "\tspeed: 0.0915s/iter; left time: 11.8059s\n",
      "\titers: 500, epoch: 10 | loss: 0.2851472\n",
      "\tspeed: 0.0918s/iter; left time: 2.6617s\n",
      "Epoch: 10 cost time: 48.62476682662964\n",
      "Epoch: 10, Steps: 528 | Train Loss: 0.3111989 Vali Loss: 0.6959119 Test Loss: 0.3759353\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.1382898093332365e-08\n",
      ">>>>>>>testing : long_term_forecast_ETTh1_96_96_none_LLMMixer_ETTh1_sl96_pl96_dm16_nh4_el2_dl1_df32_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 2785\n",
      "test shape: (174, 16, 96, 7) (174, 16, 96, 7)\n",
      "test shape: (2784, 96, 7) (2784, 96, 7)\n",
      "mse:0.3724925043582916, mae:0.3991514762401581\n",
      "rmse:0.6152174472808838, mape:0.6807839274406433, mspe:44071.46484375\n"
     ]
    }
   ],
   "source": [
    "model_name=\"LLMMixer\"\n",
    "\n",
    "seq_len=96\n",
    "e_layers=2\n",
    "down_sampling_layers=3\n",
    "down_sampling_window=2\n",
    "learning_rate=0.001\n",
    "d_model=16\n",
    "d_ff=32\n",
    "train_epochs=10\n",
    "patience=10\n",
    "batch_size=16\n",
    "\n",
    "pred_len=96\n",
    "\n",
    "!python -u run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path  /home/kowsher/tabllm \\\n",
    "  --data_path ETTh1.csv \\\n",
    "  --model_id ETTh1_$seq_len'_'96 \\\n",
    "  --model $model_name \\\n",
    "  --data ETTh1 \\\n",
    "  --features M \\\n",
    "  --seq_len $seq_len \\\n",
    "  --label_len 0 \\\n",
    "  --pred_len $pred_len \\\n",
    "  --e_layers $e_layers \\\n",
    "  --enc_in 7 \\\n",
    "  --c_out 7 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --patience $patience \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --down_sampling_layers $down_sampling_layers \\\n",
    "  --down_sampling_method avg \\\n",
    "  --down_sampling_window $down_sampling_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45ce607f-ea60-46d1-af0e-b810a4599f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTh1_192_192', model='LLMMixer', data='ETTh1', root_path='/home/kowsher/tabllm', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=192, label_len=0, pred_len=192, seasonal_patterns='Monthly', inverse=False, llm_path='FacebookAI/roberta-base', tokenizer_path='FacebookAI/roberta-base', top_k=5, num_kernels=6, enc_in=7, dec_in=7, c_out=7, d_model=16, n_heads=4, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, channel_independence=1, decomp_method='moving_avg', use_norm=1, down_sampling_layers=3, down_sampling_window=2, down_sampling_method='avg', use_future_temporal_feature=0, mask_rate=0.25, anomaly_ratio=0.25, num_workers=10, itr=1, train_epochs=10, batch_size=16, patience=3, learning_rate=0.001, des='Exp', loss='MSE', lradj='TST', pct_start=0.2, use_amp=False, comment='none', use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1', p_hidden_dims=[128, 128], p_hidden_layers=2)\n",
      "Use GPU: cuda:0\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      ">>>>>>>start training : long_term_forecast_ETTh1_192_192_none_LLMMixer_ETTh1_sl192_pl192_dm16_nh4_el2_dl1_df32_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 8257\n",
      "val 2689\n",
      "test 2689\n",
      "\titers: 100, epoch: 1 | loss: 0.9966863\n",
      "\tspeed: 0.0974s/iter; left time: 493.1252s\n",
      "\titers: 200, epoch: 1 | loss: 0.6399108\n",
      "\tspeed: 0.0921s/iter; left time: 456.8632s\n",
      "\titers: 300, epoch: 1 | loss: 0.4695871\n",
      "\tspeed: 0.0905s/iter; left time: 439.7174s\n",
      "\titers: 400, epoch: 1 | loss: 0.4585675\n",
      "\tspeed: 0.0914s/iter; left time: 435.0392s\n",
      "\titers: 500, epoch: 1 | loss: 0.4749523\n",
      "\tspeed: 0.0896s/iter; left time: 417.6283s\n",
      "Epoch: 1 cost time: 47.540117263793945\n",
      "Epoch: 1, Steps: 516 | Train Loss: 0.8010264 Vali Loss: 1.0609692 Test Loss: 0.4592998\n",
      "Validation loss decreased (inf --> 1.060969).  Saving model ...\n",
      "Updating learning rate to 0.0005207313112950188\n",
      "\titers: 100, epoch: 2 | loss: 0.3642012\n",
      "\tspeed: 0.2832s/iter; left time: 1287.3655s\n",
      "\titers: 200, epoch: 2 | loss: 0.4193079\n",
      "\tspeed: 0.0902s/iter; left time: 400.9445s\n",
      "\titers: 300, epoch: 2 | loss: 0.5094141\n",
      "\tspeed: 0.0909s/iter; left time: 394.9364s\n",
      "\titers: 400, epoch: 2 | loss: 0.4263056\n",
      "\tspeed: 0.1101s/iter; left time: 467.1894s\n",
      "\titers: 500, epoch: 2 | loss: 0.3997852\n",
      "\tspeed: 0.1352s/iter; left time: 560.2380s\n",
      "Epoch: 2 cost time: 54.117913246154785\n",
      "Epoch: 2, Steps: 516 | Train Loss: 0.4239633 Vali Loss: 0.9933020 Test Loss: 0.4399577\n",
      "Validation loss decreased (1.060969 --> 0.993302).  Saving model ...\n",
      "Updating learning rate to 0.0009999998552033152\n",
      "\titers: 100, epoch: 3 | loss: 0.3404683\n",
      "\tspeed: 0.4562s/iter; left time: 1838.0905s\n",
      "\titers: 200, epoch: 3 | loss: 0.4079774\n",
      "\tspeed: 0.1351s/iter; left time: 530.8517s\n",
      "\titers: 300, epoch: 3 | loss: 0.3608131\n",
      "\tspeed: 0.1241s/iter; left time: 475.0070s\n",
      "\titers: 400, epoch: 3 | loss: 0.3550453\n",
      "\tspeed: 0.0894s/iter; left time: 333.5354s\n",
      "\titers: 500, epoch: 3 | loss: 0.4878881\n",
      "\tspeed: 0.0894s/iter; left time: 324.6121s\n",
      "Epoch: 3 cost time: 59.39443325996399\n",
      "Epoch: 3, Steps: 516 | Train Loss: 0.4042019 Vali Loss: 1.0317575 Test Loss: 0.4500478\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0009617941657133124\n",
      "\titers: 100, epoch: 4 | loss: 0.3484591\n",
      "\tspeed: 0.2753s/iter; left time: 967.2732s\n",
      "\titers: 200, epoch: 4 | loss: 0.3881081\n",
      "\tspeed: 0.0896s/iter; left time: 305.9077s\n",
      "\titers: 300, epoch: 4 | loss: 0.3307539\n",
      "\tspeed: 0.0893s/iter; left time: 295.7763s\n",
      "\titers: 400, epoch: 4 | loss: 0.3462589\n",
      "\tspeed: 0.0897s/iter; left time: 288.2665s\n",
      "\titers: 500, epoch: 4 | loss: 0.3644413\n",
      "\tspeed: 0.0894s/iter; left time: 278.4114s\n",
      "Epoch: 4 cost time: 46.31681966781616\n",
      "Epoch: 4, Steps: 516 | Train Loss: 0.3879593 Vali Loss: 1.0673175 Test Loss: 0.4749017\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.0008532848051498643\n",
      "\titers: 100, epoch: 5 | loss: 0.3849036\n",
      "\tspeed: 0.2755s/iter; left time: 825.6339s\n",
      "\titers: 200, epoch: 5 | loss: 0.4106692\n",
      "\tspeed: 0.0892s/iter; left time: 258.5346s\n",
      "\titers: 300, epoch: 5 | loss: 0.3016635\n",
      "\tspeed: 0.0895s/iter; left time: 250.3566s\n",
      "\titers: 400, epoch: 5 | loss: 0.3413049\n",
      "\tspeed: 0.0896s/iter; left time: 241.6090s\n",
      "\titers: 500, epoch: 5 | loss: 0.3294108\n",
      "\tspeed: 0.0893s/iter; left time: 231.9651s\n",
      "Epoch: 5 cost time: 46.27278923988342\n",
      "Epoch: 5, Steps: 516 | Train Loss: 0.3720784 Vali Loss: 1.1087056 Test Loss: 0.4641872\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_ETTh1_192_192_none_LLMMixer_ETTh1_sl192_pl192_dm16_nh4_el2_dl1_df32_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 2689\n",
      "test shape: (168, 16, 192, 7) (168, 16, 192, 7)\n",
      "test shape: (2688, 192, 7) (2688, 192, 7)\n",
      "mse:0.4399578869342804, mae:0.43315309286117554\n",
      "rmse:0.6632932424545288, mape:0.695132315158844, mspe:35936.84375\n"
     ]
    }
   ],
   "source": [
    "model_name=\"LLMMixer\"\n",
    "\n",
    "seq_len=192\n",
    "e_layers=2\n",
    "down_sampling_layers=3\n",
    "down_sampling_window=2\n",
    "learning_rate=0.001\n",
    "d_model=16\n",
    "d_ff=32\n",
    "train_epochs=10\n",
    "patience=3\n",
    "batch_size=16\n",
    "\n",
    "pred_len=192\n",
    "\n",
    "!python -u run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path  /home/kowsher/tabllm \\\n",
    "  --data_path ETTh1.csv \\\n",
    "  --model_id ETTh1_$seq_len'_'$pred_len \\\n",
    "  --model $model_name \\\n",
    "  --data ETTh1 \\\n",
    "  --features M \\\n",
    "  --seq_len $seq_len \\\n",
    "  --label_len 0 \\\n",
    "  --pred_len $pred_len \\\n",
    "  --e_layers $e_layers \\\n",
    "  --enc_in 7 \\\n",
    "  --c_out 7 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --patience $patience \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --down_sampling_layers $down_sampling_layers \\\n",
    "  --down_sampling_method avg \\\n",
    "  --down_sampling_window $down_sampling_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf223a9c-ffa7-4baa-bc22-c9f603c5766a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTh1_192_336', model='LLMMixer', data='ETTh1', root_path='/home/kowsher/tabllm', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=192, label_len=0, pred_len=336, seasonal_patterns='Monthly', inverse=False, llm_path='FacebookAI/roberta-base', tokenizer_path='FacebookAI/roberta-base', top_k=5, num_kernels=6, enc_in=7, dec_in=7, c_out=7, d_model=16, n_heads=4, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, channel_independence=1, decomp_method='moving_avg', use_norm=1, down_sampling_layers=5, down_sampling_window=1, down_sampling_method='avg', use_future_temporal_feature=0, mask_rate=0.25, anomaly_ratio=0.25, num_workers=10, itr=1, train_epochs=10, batch_size=16, patience=3, learning_rate=0.001, des='Exp', loss='MSE', lradj='TST', pct_start=0.2, use_amp=False, comment='none', use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1', p_hidden_dims=[128, 128], p_hidden_layers=2)\n",
      "Use GPU: cuda:0\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      ">>>>>>>start training : long_term_forecast_ETTh1_192_336_none_LLMMixer_ETTh1_sl192_pl336_dm16_nh4_el2_dl1_df32_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 8113\n",
      "val 2545\n",
      "test 2545\n",
      "\titers: 100, epoch: 1 | loss: 0.6613762\n",
      "\tspeed: 0.1028s/iter; left time: 511.0081s\n",
      "\titers: 200, epoch: 1 | loss: 0.5772955\n",
      "\tspeed: 0.0955s/iter; left time: 465.2396s\n",
      "\titers: 300, epoch: 1 | loss: 0.5283070\n",
      "\tspeed: 0.0953s/iter; left time: 454.8394s\n",
      "\titers: 400, epoch: 1 | loss: 0.4143936\n",
      "\tspeed: 0.0998s/iter; left time: 466.2938s\n",
      "\titers: 500, epoch: 1 | loss: 0.4959140\n",
      "\tspeed: 0.1026s/iter; left time: 469.1794s\n",
      "Epoch: 1 cost time: 50.395407915115356\n",
      "Epoch: 1, Steps: 507 | Train Loss: 0.6014549 Vali Loss: 1.2531588 Test Loss: 0.4636695\n",
      "Validation loss decreased (inf --> 1.253159).  Saving model ...\n",
      "Updating learning rate to 0.0005207443059572615\n",
      "\titers: 100, epoch: 2 | loss: 0.4293335\n",
      "\tspeed: 0.2881s/iter; left time: 1286.1202s\n",
      "\titers: 200, epoch: 2 | loss: 0.4855868\n",
      "\tspeed: 0.0944s/iter; left time: 412.0822s\n",
      "\titers: 300, epoch: 2 | loss: 0.4594597\n",
      "\tspeed: 0.0941s/iter; left time: 401.3770s\n",
      "\titers: 400, epoch: 2 | loss: 0.4014509\n",
      "\tspeed: 0.0962s/iter; left time: 400.6063s\n",
      "\titers: 500, epoch: 2 | loss: 0.4765221\n",
      "\tspeed: 0.0935s/iter; left time: 380.1554s\n",
      "Epoch: 2 cost time: 48.34765601158142\n",
      "Epoch: 2, Steps: 507 | Train Loss: 0.4682487 Vali Loss: 1.3428409 Test Loss: 0.4830758\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.000999999850016977\n",
      "\titers: 100, epoch: 3 | loss: 0.4719576\n",
      "\tspeed: 0.2640s/iter; left time: 1044.6271s\n",
      "\titers: 200, epoch: 3 | loss: 0.5092761\n",
      "\tspeed: 0.0910s/iter; left time: 351.1639s\n",
      "\titers: 300, epoch: 3 | loss: 0.5366782\n",
      "\tspeed: 0.0924s/iter; left time: 347.0954s\n",
      "\titers: 400, epoch: 3 | loss: 0.3560992\n",
      "\tspeed: 0.0905s/iter; left time: 331.0415s\n",
      "\titers: 500, epoch: 3 | loss: 0.4352397\n",
      "\tspeed: 0.0909s/iter; left time: 323.3400s\n",
      "Epoch: 3 cost time: 46.40042591094971\n",
      "Epoch: 3, Steps: 507 | Train Loss: 0.4377027 Vali Loss: 1.3898887 Test Loss: 0.5020019\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.0009617915759694584\n",
      "\titers: 100, epoch: 4 | loss: 0.4363571\n",
      "\tspeed: 0.2614s/iter; left time: 901.8942s\n",
      "\titers: 200, epoch: 4 | loss: 0.4393252\n",
      "\tspeed: 0.0966s/iter; left time: 323.6297s\n",
      "\titers: 300, epoch: 4 | loss: 0.3790915\n",
      "\tspeed: 0.0940s/iter; left time: 305.5370s\n",
      "\titers: 400, epoch: 4 | loss: 0.3429470\n",
      "\tspeed: 0.0935s/iter; left time: 294.5063s\n",
      "\titers: 500, epoch: 4 | loss: 0.3416849\n",
      "\tspeed: 0.0913s/iter; left time: 278.5914s\n",
      "Epoch: 4 cost time: 47.5369291305542\n",
      "Epoch: 4, Steps: 507 | Train Loss: 0.3926937 Vali Loss: 1.4221299 Test Loss: 0.5048277\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_ETTh1_192_336_none_LLMMixer_ETTh1_sl192_pl336_dm16_nh4_el2_dl1_df32_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 2545\n",
      "test shape: (159, 16, 336, 7) (159, 16, 336, 7)\n",
      "test shape: (2544, 336, 7) (2544, 336, 7)\n",
      "mse:0.4636695683002472, mae:0.4427492022514343\n",
      "rmse:0.680932879447937, mape:0.7028952836990356, mspe:38021.10546875\n"
     ]
    }
   ],
   "source": [
    "model_name=\"LLMMixer\"\n",
    "\n",
    "seq_len=192\n",
    "e_layers=2\n",
    "down_sampling_layers=5\n",
    "down_sampling_window=1\n",
    "learning_rate=0.001\n",
    "d_model=16\n",
    "d_ff=32\n",
    "train_epochs=10\n",
    "patience=3\n",
    "batch_size=16\n",
    "\n",
    "pred_len=336\n",
    "\n",
    "!python -u run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path  /home/kowsher/tabllm \\\n",
    "  --data_path ETTh1.csv \\\n",
    "  --model_id ETTh1_$seq_len'_'$pred_len \\\n",
    "  --model $model_name \\\n",
    "  --data ETTh1 \\\n",
    "  --features M \\\n",
    "  --seq_len $seq_len \\\n",
    "  --label_len 0 \\\n",
    "  --pred_len $pred_len \\\n",
    "  --e_layers $e_layers \\\n",
    "  --enc_in 7 \\\n",
    "  --c_out 7 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --patience $patience \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --down_sampling_layers $down_sampling_layers \\\n",
    "  --down_sampling_method avg \\\n",
    "  --down_sampling_window $down_sampling_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "684962e0-3349-4f8e-86ba-9ce870a74002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(task_name='long_term_forecast', is_training=1, model_id='ETTh1_192_720', model='LLMMixer', data='ETTh1', root_path='/home/kowsher/tabllm', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=192, label_len=0, pred_len=720, seasonal_patterns='Monthly', inverse=False, llm_path='FacebookAI/roberta-base', tokenizer_path='FacebookAI/roberta-base', top_k=5, num_kernels=6, enc_in=7, dec_in=7, c_out=7, d_model=16, n_heads=4, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, channel_independence=1, decomp_method='moving_avg', use_norm=1, down_sampling_layers=4, down_sampling_window=2, down_sampling_method='avg', use_future_temporal_feature=0, mask_rate=0.25, anomaly_ratio=0.25, num_workers=10, itr=1, train_epochs=10, batch_size=16, patience=2, learning_rate=0.001, des='Exp', loss='MSE', lradj='TST', pct_start=0.2, use_amp=False, comment='none', use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1', p_hidden_dims=[128, 128], p_hidden_layers=2)\n",
      "Use GPU: cuda:0\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      ">>>>>>>start training : long_term_forecast_ETTh1_192_720_none_LLMMixer_ETTh1_sl192_pl720_dm16_nh4_el2_dl1_df32_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 7729\n",
      "val 2161\n",
      "test 2161\n",
      "\titers: 100, epoch: 1 | loss: 0.7513301\n",
      "\tspeed: 0.0992s/iter; left time: 469.0840s\n",
      "\titers: 200, epoch: 1 | loss: 0.6793501\n",
      "\tspeed: 0.0915s/iter; left time: 423.7246s\n",
      "\titers: 300, epoch: 1 | loss: 0.5844911\n",
      "\tspeed: 0.0909s/iter; left time: 411.9477s\n",
      "\titers: 400, epoch: 1 | loss: 0.7359504\n",
      "\tspeed: 0.0948s/iter; left time: 420.2706s\n",
      "Epoch: 1 cost time: 45.33447027206421\n",
      "Epoch: 1, Steps: 483 | Train Loss: 0.7179469 Vali Loss: 1.5494361 Test Loss: 0.5027507\n",
      "Validation loss decreased (inf --> 1.549436).  Saving model ...\n",
      "Updating learning rate to 0.0005207813283978231\n",
      "\titers: 100, epoch: 2 | loss: 0.6940550\n",
      "\tspeed: 0.3225s/iter; left time: 1370.1787s\n",
      "\titers: 200, epoch: 2 | loss: 0.6352818\n",
      "\tspeed: 0.0938s/iter; left time: 388.9977s\n",
      "\titers: 300, epoch: 2 | loss: 0.6375989\n",
      "\tspeed: 0.0945s/iter; left time: 382.3417s\n",
      "\titers: 400, epoch: 2 | loss: 0.6311040\n",
      "\tspeed: 0.0925s/iter; left time: 365.1206s\n",
      "Epoch: 2 cost time: 45.32344198226929\n",
      "Epoch: 2, Steps: 483 | Train Loss: 0.6047495 Vali Loss: 1.5406664 Test Loss: 0.4764489\n",
      "Validation loss decreased (1.549436 --> 1.540666).  Saving model ...\n",
      "Updating learning rate to 0.0009999998347415186\n",
      "\titers: 100, epoch: 3 | loss: 0.6193501\n",
      "\tspeed: 0.3230s/iter; left time: 1216.0762s\n",
      "\titers: 200, epoch: 3 | loss: 0.6377046\n",
      "\tspeed: 0.0946s/iter; left time: 346.8831s\n",
      "\titers: 300, epoch: 3 | loss: 0.6240880\n",
      "\tspeed: 0.0960s/iter; left time: 342.1215s\n",
      "\titers: 400, epoch: 3 | loss: 0.6001618\n",
      "\tspeed: 0.0931s/iter; left time: 322.5778s\n",
      "Epoch: 3 cost time: 45.82100248336792\n",
      "Epoch: 3, Steps: 483 | Train Loss: 0.5846128 Vali Loss: 1.6193384 Test Loss: 0.5645067\n",
      "EarlyStopping counter: 1 out of 2\n",
      "Updating learning rate to 0.0009617841976863903\n",
      "\titers: 100, epoch: 4 | loss: 0.4549142\n",
      "\tspeed: 0.3227s/iter; left time: 1059.1438s\n",
      "\titers: 200, epoch: 4 | loss: 0.4946029\n",
      "\tspeed: 0.0923s/iter; left time: 293.7810s\n",
      "\titers: 300, epoch: 4 | loss: 0.5389666\n",
      "\tspeed: 0.0939s/iter; left time: 289.3103s\n",
      "\titers: 400, epoch: 4 | loss: 0.4427575\n",
      "\tspeed: 0.0921s/iter; left time: 274.5376s\n",
      "Epoch: 4 cost time: 45.05303716659546\n",
      "Epoch: 4, Steps: 483 | Train Loss: 0.5421340 Vali Loss: 1.6378524 Test Loss: 0.5916071\n",
      "EarlyStopping counter: 2 out of 2\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_ETTh1_192_720_none_LLMMixer_ETTh1_sl192_pl720_dm16_nh4_el2_dl1_df32_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 2161\n",
      "test shape: (135, 16, 720, 7) (135, 16, 720, 7)\n",
      "test shape: (2160, 720, 7) (2160, 720, 7)\n",
      "mse:0.47644901275634766, mae:0.4764063358306885\n",
      "rmse:0.6902528405189514, mape:0.7453890442848206, mspe:48781.10546875\n"
     ]
    }
   ],
   "source": [
    "model_name=\"LLMMixer\"\n",
    "\n",
    "seq_len=192\n",
    "e_layers=2\n",
    "down_sampling_layers=4\n",
    "down_sampling_window=2\n",
    "learning_rate=0.001\n",
    "d_model=16\n",
    "d_ff=32\n",
    "train_epochs=10\n",
    "patience=2\n",
    "batch_size=16\n",
    "\n",
    "pred_len=720\n",
    "\n",
    "!python -u run.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path  /home/kowsher/tabllm \\\n",
    "  --data_path ETTh1.csv \\\n",
    "  --model_id ETTh1_$seq_len'_'$pred_len \\\n",
    "  --model $model_name \\\n",
    "  --data ETTh1 \\\n",
    "  --features M \\\n",
    "  --seq_len $seq_len \\\n",
    "  --label_len 0 \\\n",
    "  --pred_len $pred_len \\\n",
    "  --e_layers $e_layers \\\n",
    "  --enc_in 7 \\\n",
    "  --c_out 7 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --d_model $d_model \\\n",
    "  --d_ff $d_ff \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --train_epochs $train_epochs \\\n",
    "  --patience $patience \\\n",
    "  --batch_size $batch_size \\\n",
    "  --learning_rate $learning_rate \\\n",
    "  --down_sampling_layers $down_sampling_layers \\\n",
    "  --down_sampling_method avg \\\n",
    "  --down_sampling_window $down_sampling_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190f811b-1345-471d-ab1e-78782a771667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
